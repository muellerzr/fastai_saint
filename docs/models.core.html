---

title: Model Foundations


keywords: fastai
sidebar: home_sidebar

summary: "Core aspects to building a `SAINT` model"
description: "Core aspects to building a `SAINT` model"
nb_path: "02_models.core.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 02_models.core.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The history saving thread hit an unexpected error (DatabaseError(&#39;database disk image is malformed&#39;)).History will not be written to the database.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/mnt/d/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() &gt; 0
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Helper-Functions">Helper Functions<a class="anchor-link" href="#Helper-Functions"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="exists" class="doc_header"><code>exists</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L21" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>exists</code>(<strong><code>val</code></strong>)</p>
</blockquote>
<p>Tests if <code>val</code> is not None</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>val</code>: Any value</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="default" class="doc_header"><code>default</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L28" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>default</code>(<strong><code>val</code></strong>, <strong><code>d</code></strong>)</p>
</blockquote>
<p>Get a default if <code>val</code> doesn't exist</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>val</code>: Any value</li>
<li><code>d</code>: Some default for <code>val</code></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="ff_encodings" class="doc_header"><code>ff_encodings</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L36" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>ff_encodings</code>(<strong><code>x</code></strong>:<code>tensor</code>, <strong><code>B</code></strong>:<code>tensor</code>)</p>
</blockquote>
<p>Return sin and cosine projections of <code>x</code> @ <code>B</code></p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>x</code> (<code>torch.tensor</code>): Input</li>
<li><code>B</code> (<code>torch.tensor</code>): Projection matrix</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Basic-Layers">Basic Layers<a class="anchor-link" href="#Basic-Layers"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Residual" class="doc_header"><code>class</code> <code>Residual</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L45" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Residual</code>(<strong><code>fn</code></strong>:<code>callable</code>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>fn</code> (<code>callable</code>): A function to generate a residual</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Residual.__init__" class="doc_header"><code>Residual.__init__</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L46" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Residual.__init__</code>(<strong><code>fn</code></strong>:<code>callable</code>)</p>
</blockquote>
<p>A layer that adds a residual <code>fn</code> to x</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>fn</code> (<code>callable</code>): A function to generate a residual</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Residual.forward" class="doc_header"><code>Residual.forward</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L53" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Residual.forward</code>(<strong><code>x</code></strong>:<code>tensor</code>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Applies <code>self.fn</code> to x</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>x</code> (<code>torch.tensor</code>): An input</li>
<li><code>**kwargs</code>: kwargs for <code>self.fn</code></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PreNorm" class="doc_header"><code>class</code> <code>PreNorm</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L62" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PreNorm</code>(<strong><code>dim</code></strong>:<code>int</code>, <strong><code>fn</code></strong>:<code>callable</code>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>dim</code> (<code>int</code>): LayerNorm dimension</li>
<li><code>fn</code> (<code>callable</code>): Residual function</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="PreNorm.__init__" class="doc_header"><code>PreNorm.__init__</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L63" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>PreNorm.__init__</code>(<strong><code>dim</code></strong>:<code>int</code>, <strong><code>fn</code></strong>:<code>callable</code>)</p>
</blockquote>
<p>Layer norm that is applied before calling a residual function</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>dim</code> (<code>int</code>): LayerNorm dimension</li>
<li><code>fn</code> (<code>callable</code>): Residual function</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="PreNorm.forward" class="doc_header"><code>PreNorm.forward</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L72" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>PreNorm.forward</code>(<strong><code>x</code></strong>:<code>tensor</code>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Applies <code>self.fn</code> to the output of <code>self.norm(x)</code></p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>x</code> (<code>torch.tensor</code>): An input</li>
<li><code>**kwargs</code>: kwargs for <code>self.fn</code></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Attention-Layers">Attention Layers<a class="anchor-link" href="#Attention-Layers"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GEGLU" class="doc_header"><code>class</code> <code>GEGLU</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L81" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GEGLU</code>() :: <code>Module</code></p>
</blockquote>
<p>GLU variation introduced in GLU Variants Improve Transformer</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GEGLU.forward" class="doc_header"><code>GEGLU.forward</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L83" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GEGLU.forward</code>(<strong><code>x</code></strong>:<code>tensor</code>)</p>
</blockquote>
<p>Chunks <code>x</code> into 2, applies <code>F.gelu</code> to chunks, and returns product with <code>x</code></p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>x</code> (<code>torch.tensor</code>): An input</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="FeedForward" class="doc_header"><code>class</code> <code>FeedForward</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L92" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>FeedForward</code>(<strong><code>dim</code></strong>:<code>int</code>, <strong><code>mult</code></strong>:<code>int</code>=<em><code>4</code></em>, <strong><code>dropout</code></strong>:<code>(&lt;class 'int'&gt;, &lt;class 'float'&gt;)</code>=<em><code>0.0</code></em>) :: <code>Sequential</code></p>
</blockquote>
<p>A sequential container.
Modules will be added to it in the order they are passed in the constructor.
Alternatively, an ordered dict of modules can also be passed in.</p>
<p>To make it easier to understand, here is a small example::</p>

<pre><code># Example of using Sequential
model = nn.Sequential(
          nn.Conv2d(1,20,5),
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )

# Example of using Sequential with OrderedDict
model = nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(1,20,5)),
          ('relu1', nn.ReLU()),
          ('conv2', nn.Conv2d(20,64,5)),
          ('relu2', nn.ReLU())
        ]))

</code></pre>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>dim</code> (<code>int</code>): Linear layer dimension</li>
<li><code>mult</code> (<code>int</code>): Multiplier for <code>dim</code></li>
<li><code>dropout</code> (<code>int,float)</code>): Dropout probability</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="FeedForward.forward" class="doc_header"><code>FeedForward.forward</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L108" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>FeedForward.forward</code>(<strong><code>x</code></strong>:<code>tensor</code>)</p>
</blockquote>
<p>Applies x to a <code>Linear</code> -&gt; <a href="/fastai_saint/models.core.html#GEGLU"><code>GEGLU</code></a> -&gt; <code>Dropout</code> -&gt; <code>Linear</code> group</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>x</code> (<code>torch.tensor</code>): An input</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Attention" class="doc_header"><code>class</code> <code>Attention</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L116" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Attention</code>(<strong><code>dim</code></strong>:<code>int</code>, <strong><code>heads</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>dim_head</code></strong>:<code>int</code>=<em><code>16</code></em>, <strong><code>dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>dim</code> (<code>int</code>): Dimension for the Linear groups</li>
<li><code>heads</code> (<code>int</code>): Number of attention heads</li>
<li><code>dim_head</code> (<code>int</code>): Dimension of the attention heads</li>
<li><code>dropout</code> (<code>float</code>): Dropout probability</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Attention.__init__" class="doc_header"><code>Attention.__init__</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L117" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Attention.__init__</code>(<strong><code>dim</code></strong>:<code>int</code>, <strong><code>heads</code></strong>:<code>int</code>=<em><code>8</code></em>, <strong><code>dim_head</code></strong>:<code>int</code>=<em><code>16</code></em>, <strong><code>dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>)</p>
</blockquote>
<p>A basic attention wrt module</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>dim</code> (<code>int</code>): Dimension for the Linear groups</li>
<li><code>heads</code> (<code>int</code>): Number of attention heads</li>
<li><code>dim_head</code> (<code>int</code>): Dimension of the attention heads</li>
<li><code>dropout</code> (<code>float</code>): Dropout probability</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Attention.forward" class="doc_header"><code>Attention.forward</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L134" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Attention.forward</code>(<strong><code>x</code></strong>:<code>tensor</code>)</p>
</blockquote>
<p>Applies attention to <code>x</code></p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>x</code> (<code>torch.tensor</code>): An input</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="AttentionType" class="doc_header"><code>class</code> <code>AttentionType</code><a href="" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>AttentionType</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>All possible attention types, with typo-proofing and auto-complete</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RowColAttention" class="doc_header"><code>class</code> <code>RowColAttention</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L153" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RowColAttention</code>(<strong><code>num_tokens</code></strong>:<code>int</code>, <strong><code>dim</code></strong>:<code>int</code>, <strong><code>nfeats</code></strong>:<code>int</code>, <strong><code>depth</code></strong>:<code>int</code>, <strong><code>heads</code></strong>:<code>int</code>, <strong><code>dim_head</code></strong>:<code>int</code>=<em><code>16</code></em>, <strong><code>attn_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>ff_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>style</code></strong>:<code>AttentionType</code>=<em><code>'col'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>num_tokens</code> (<code>int</code>): Size of the categorical embeddings</li>
<li><code>dim</code> (<code>int</code>): Dimension of the two Embedding layers</li>
<li><code>nfeats</code> (<code>int</code>): Number of continuous features</li>
<li><code>depth</code> (<code>int</code>): The number of attention modules to generate</li>
<li><code>heads</code> (<code>int</code>): Number of attention heads</li>
<li><code>dim_head</code> (<code>int</code>): Dimension of the attention heads</li>
<li><code>attn_dropout</code> (<code>float</code>): Dropout probability in the attention module</li>
<li><code>ff_dropout</code> (<code>float</code>): Dropout probability for the feed forward layers</li>
<li><code>style</code> (<code>AttentionType</code>): Attention style</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="RowColAttention.__init__" class="doc_header"><code>RowColAttention.__init__</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L154" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>RowColAttention.__init__</code>(<strong><code>num_tokens</code></strong>:<code>int</code>, <strong><code>dim</code></strong>:<code>int</code>, <strong><code>nfeats</code></strong>:<code>int</code>, <strong><code>depth</code></strong>:<code>int</code>, <strong><code>heads</code></strong>:<code>int</code>, <strong><code>dim_head</code></strong>:<code>int</code>=<em><code>16</code></em>, <strong><code>attn_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>ff_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>style</code></strong>:<code>AttentionType</code>=<em><code>'col'</code></em>)</p>
</blockquote>
<p>A small Transformer that generates attention based on a row and column</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>num_tokens</code> (<code>int</code>): Size of the categorical embeddings</li>
<li><code>dim</code> (<code>int</code>): Dimension of the two Embedding layers</li>
<li><code>nfeats</code> (<code>int</code>): Number of continuous features</li>
<li><code>depth</code> (<code>int</code>): The number of attention modules to generate</li>
<li><code>heads</code> (<code>int</code>): Number of attention heads</li>
<li><code>dim_head</code> (<code>int</code>): Dimension of the attention heads</li>
<li><code>attn_dropout</code> (<code>float</code>): Dropout probability in the attention module</li>
<li><code>ff_dropout</code> (<code>float</code>): Dropout probability for the feed forward layers</li>
<li><code>style</code> (<code>AttentionType</code>): Attention style</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="RowColAttention.forward" class="doc_header"><code>RowColAttention.forward</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L187" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>RowColAttention.forward</code>(<strong><code>x</code></strong>, <strong><code>x_cont</code></strong>=<em><code>None</code></em>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Applies an attention mechanism on inputs</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>x</code>: Categorical inputs</li>
<li><code>x_cont</code>: Continuous inputs</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Transformer" class="doc_header"><code>class</code> <code>Transformer</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L214" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Transformer</code>(<strong><code>num_tokens</code></strong>:<code>int</code>, <strong><code>dim</code></strong>:<code>int</code>, <strong><code>depth</code></strong>:<code>int</code>, <strong><code>heads</code></strong>:<code>int</code>, <strong><code>dim_head</code></strong>:<code>int</code>=<em><code>16</code></em>, <strong><code>attn_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>ff_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>num_tokens</code> (<code>int</code>): Size of the categorical embeddings in the <code>Attention</code> layer</li>
<li><code>dim</code> (<code>int</code>): Dimension of the two Embedding layers in the <code>Attention</code> layer</li>
<li><code>depth</code> (<code>int</code>): The number of attention modules to generate in the <code>Attention</code> layer</li>
<li><code>heads</code> (<code>int</code>): Number of attention heads in the <code>Attention</code> layer</li>
<li><code>dim_head</code> (<code>int</code>): Dimension of the attention heads in the <code>Attention</code> layer</li>
<li><code>attn_dropout</code> (<code>float</code>): Dropout probability in the <code>Attention</code> layer</li>
<li><code>ff_dropout</code> (<code>float</code>): Dropout probability for the feed forward layers</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Transformer.__init__" class="doc_header"><code>Transformer.__init__</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L215" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Transformer.__init__</code>(<strong><code>num_tokens</code></strong>:<code>int</code>, <strong><code>dim</code></strong>:<code>int</code>, <strong><code>depth</code></strong>:<code>int</code>, <strong><code>heads</code></strong>:<code>int</code>, <strong><code>dim_head</code></strong>:<code>int</code>=<em><code>16</code></em>, <strong><code>attn_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>ff_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>)</p>
</blockquote>
<p>A basic feed forward Transformer module with attention</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>num_tokens</code> (<code>int</code>): Size of the categorical embeddings in the <code>Attention</code> layer</li>
<li><code>dim</code> (<code>int</code>): Dimension of the two Embedding layers in the <code>Attention</code> layer</li>
<li><code>depth</code> (<code>int</code>): The number of attention modules to generate in the <code>Attention</code> layer</li>
<li><code>heads</code> (<code>int</code>): Number of attention heads in the <code>Attention</code> layer</li>
<li><code>dim_head</code> (<code>int</code>): Dimension of the attention heads in the <code>Attention</code> layer</li>
<li><code>attn_dropout</code> (<code>float</code>): Dropout probability in the <code>Attention</code> layer</li>
<li><code>ff_dropout</code> (<code>float</code>): Dropout probability for the feed forward layers</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Transformer.forward" class="doc_header"><code>Transformer.forward</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L235" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Transformer.forward</code>(<strong><code>x</code></strong>, <strong><code>x_cont</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Applies attention to inputs</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>x</code>: Categorical inputs</li>
<li><code>x_cont</code>: Continuous inputs</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MLP" class="doc_header"><code>class</code> <code>MLP</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L249" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MLP</code>(<strong><code>dims</code></strong>:<code>list</code>, <strong><code>act</code></strong>=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>dims</code> (<code>list</code>): A list of dimensions for the module</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MLP.__init__" class="doc_header"><code>MLP.__init__</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L250" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MLP.__init__</code>(<strong><code>dims</code></strong>:<code>list</code>, <strong><code>act</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>A basic multi-layer perceptron module</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>dims</code> (<code>list</code>): A list of dimensions for the module</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MLP.forward" class="doc_header"><code>MLP.forward</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L270" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MLP.forward</code>(<strong><code>x</code></strong>:<code>tensor</code>)</p>
</blockquote>
<p>Applies mlp on <code>x</code></p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>x</code> (<code>torch.tensor</code>): An input</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SimpleMLP" class="doc_header"><code>class</code> <code>SimpleMLP</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L278" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SimpleMLP</code>(<strong><code>dims</code></strong>:<code>list</code>) :: <code>Sequential</code></p>
</blockquote>
<p>A sequential container.
Modules will be added to it in the order they are passed in the constructor.
Alternatively, an ordered dict of modules can also be passed in.</p>
<p>To make it easier to understand, here is a small example::</p>

<pre><code># Example of using Sequential
model = nn.Sequential(
          nn.Conv2d(1,20,5),
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )

# Example of using Sequential with OrderedDict
model = nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(1,20,5)),
          ('relu1', nn.ReLU()),
          ('conv2', nn.Conv2d(20,64,5)),
          ('relu2', nn.ReLU())
        ]))

</code></pre>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>dims</code> (<code>list</code>): A list of three dimensions for our MLP module</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="SimpleMLP.__init__" class="doc_header"><code>SimpleMLP.__init__</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L279" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>SimpleMLP.__init__</code>(<strong><code>dims</code></strong>:<code>list</code>)</p>
</blockquote>
<p>Simplified multi-layer perceptron</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>dims</code> (<code>list</code>): A list of three dimensions for our MLP module</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="SimpleMLP.forward" class="doc_header"><code>SimpleMLP.forward</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L291" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>SimpleMLP.forward</code>(<strong><code>x</code></strong>:<code>tensor</code>)</p>
</blockquote>
<p>Applies simplified MLP on <code>x</code></p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>x</code> (<code>torch.tensor</code>): A tensor input</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TabAttention" class="doc_header"><code>class</code> <code>TabAttention</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L304" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TabAttention</code>(<strong><code>categories</code></strong>:<code>list</code>, <strong><code>num_continuous</code></strong>:<code>int</code>, <strong><code>dim</code></strong>:<code>int</code>, <strong><code>depth</code></strong>:<code>int</code>, <strong><code>heads</code></strong>:<code>int</code>, <strong><code>dim_head</code></strong>:<code>int</code>=<em><code>16</code></em>, <strong><code>dim_out</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>mlp_hidden_mults</code></strong>:<code>Tuple</code>[<code>int</code>]=<em><code>(4, 2)</code></em>, <strong><code>mlp_act</code></strong>=<em><code>None</code></em>, <strong><code>num_special_tokens</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>attn_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>ff_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>lastmlp_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>cont_embeddings</code></strong>:<code>str</code>=<em><code>'MLP'</code></em>, <strong><code>attention_style</code></strong>:<code>AttentionType</code>=<em><code>'col'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>categories</code> (<code>list</code>): List of categorical cardinalities</li>
<li><code>num_continuous</code> (<code>int</code>): Number of continuous variables</li>
<li><code>dim</code> (<code>int</code>): MLP dimension</li>
<li><code>depth</code> (<code>int</code>): Transformer depth</li>
<li><code>heads</code> (<code>int</code>): Number of attention heads</li>
<li><code>dim_head</code> (<code>int</code>): Size of the attention head</li>
<li><code>dim_out</code> (<code>int</code>): Size of the last linear layer</li>
<li><code>mlp_hidden_mults</code> (<code>Tuple[int]</code>): Multipliers for the MLP hidden layers</li>
<li><code>num_special_tokens</code> (<code>int</code>): Number of special tokens for the categories</li>
<li><code>attn_dropout</code> (<code>float</code>): Dropout probability for the attention module</li>
<li><code>ff_dropout</code> (<code>float</code>): Dropout probability for the feed forward layers</li>
<li><code>lastmlp_dropout</code> (<code>float</code>): Dropout probability for the final MLP group</li>
<li><code>cont_embeddings</code> (<code>str</code>): Type of embeddings for the continouous variables, only <code>MLP</code> is available. If <code>None</code> no attention is applied</li>
<li><code>attention_style</code> (<code>AttentionType</code>): Attention style</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="TabAttention.__init__" class="doc_header"><code>TabAttention.__init__</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L305" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>TabAttention.__init__</code>(<strong><code>categories</code></strong>:<code>list</code>, <strong><code>num_continuous</code></strong>:<code>int</code>, <strong><code>dim</code></strong>:<code>int</code>, <strong><code>depth</code></strong>:<code>int</code>, <strong><code>heads</code></strong>:<code>int</code>, <strong><code>dim_head</code></strong>:<code>int</code>=<em><code>16</code></em>, <strong><code>dim_out</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>mlp_hidden_mults</code></strong>:<code>Tuple</code>[<code>int</code>]=<em><code>(4, 2)</code></em>, <strong><code>mlp_act</code></strong>=<em><code>None</code></em>, <strong><code>num_special_tokens</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>attn_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>ff_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>lastmlp_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>cont_embeddings</code></strong>:<code>str</code>=<em><code>'MLP'</code></em>, <strong><code>attention_style</code></strong>:<code>AttentionType</code>=<em><code>'col'</code></em>)</p>
</blockquote>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>categories</code> (<code>list</code>): List of categorical cardinalities</li>
<li><code>num_continuous</code> (<code>int</code>): Number of continuous variables</li>
<li><code>dim</code> (<code>int</code>): MLP dimension</li>
<li><code>depth</code> (<code>int</code>): Transformer depth</li>
<li><code>heads</code> (<code>int</code>): Number of attention heads</li>
<li><code>dim_head</code> (<code>int</code>): Size of the attention head</li>
<li><code>dim_out</code> (<code>int</code>): Size of the last linear layer</li>
<li><code>mlp_hidden_mults</code> (<code>Tuple[int]</code>): Multipliers for the MLP hidden layers</li>
<li><code>num_special_tokens</code> (<code>int</code>): Number of special tokens for the categories</li>
<li><code>attn_dropout</code> (<code>float</code>): Dropout probability for the attention module</li>
<li><code>ff_dropout</code> (<code>float</code>): Dropout probability for the feed forward layers</li>
<li><code>lastmlp_dropout</code> (<code>float</code>): Dropout probability for the final MLP group</li>
<li><code>cont_embeddings</code> (<code>str</code>): Type of embeddings for the continouous variables, only <code>MLP</code> is available. If <code>None</code> no attention is applied</li>
<li><code>attention_style</code> (<code>AttentionType</code>): Attention style</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="TabAttention.forward" class="doc_header"><code>TabAttention.forward</code><a href="https://github.com/muellerzr/fastai_saint/tree/main/fastai_saint/models/core.py#L399" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>TabAttention.forward</code>(<strong><code>x_categ</code></strong>:<code>tensor</code>, <strong><code>x_cont</code></strong>:<code>tensor</code>, <strong><code>x_categ_enc</code></strong>:<code>tensor</code>, <strong><code>x_cont_enc</code></strong>:<code>tensor</code>)</p>
</blockquote>
<p>Feed input through Tabular Attention</p>
<p><strong>Function Arguments</strong>:</p>
<ul>
<li><code>x_categ</code> (<code>torch.tensor</code>): Categorical inputs</li>
<li><code>x_cont</code> (<code>torch.tensor</code>): Continous inputs</li>
<li><code>x_categ_enc</code> (<code>torch.tensor</code>): Encoded categorical inputs via <code>embed_data_mask</code></li>
<li><code>x_cont_enc</code> (<code>torch.tensor</code>): Encoded continuous inputs via <code>embed_data_mask</code></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

